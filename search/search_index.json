{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to microlux","text":"<p>microlux is a Jax-based package that is used to calculate the binary lensing light curve with the finite source effect using the contour integration method.  We inherit the novel features in VBBinaryLensing including parabolic correction and optimal sampling to maximize the performance. This is built on the JAX library which provides a NumPy-like interface with GPU and automatic differentiation support for high-performance machine learning research. Through automatic differentiation and our package, we get access to the accurate gradient for exploring more advanced algorithms.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Optimal sampling and contour integration with error control to calculate the binary lens microlensing light curve with finite source effect.</li> <li>Robust and accurate calculation: Widely test over the broad parameter space compared with VBBinaryLensing</li> <li>Fast speed: Fully compatible with JIT compilation to speed up calculation. </li> <li>Accurate gradient: Automatic differentiation with novel error estimator to ensure the convergence of gradient. </li> <li>Aberth\u2013Ehrlich method to find the roots of the polynomial and liner sum assignment algorithm to match the images</li> <li>Application on real events modeling using NUTS in Numpyro</li> </ul>"},{"location":"#reference","title":"Reference","text":"<p>Under development and Paper is coming soon.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Under development</p>"},{"location":"api/model/","title":"Magnification Model","text":"<p>Light curve for the binary lens with adaptive contour integration and point sorce approximation. </p> <p>Light curve with point source approximation. </p> <p>Adaptive contour integration for the light curve. </p> <p>Adaptive contour integration for the single epoch. </p>"},{"location":"api/model/#microlux.binary_mag","title":"<code>microlux.binary_mag(t_0, u_0, t_E, rho, q, s, alpha_deg, times: jnp.ndarray, tol=0.01, retol=0.001, default_strategy: Tuple[int] = (30, 30, 60, 120, 240), analytic: bool = True, return_info: bool = False, limb_darkening_coeff: float | None = None)</code>","text":"<p>Compute the light curve of a binary lens system with finite source effects. This function will dynamically choose full contour integration or point source approximation based on the quadrupole test.</p> <p>Note</p> <p>The coordinate system is consistent with the MulensModel(Center of mass).</p> <p>Warning</p> <p>Currently, to deal with limb-darkening effect, we only use 10 annuli which are uniformly distributed in terms of the area, which is not in an adaptive scheme. So the tolerance of the limb darkening effect is not guaranteed.</p> <p>Parameters</p> <ul> <li><code>t_0</code>: The time of the peak of the microlensing event.</li> <li><code>u_0</code>: The impact parameter of the source trajectory.</li> <li><code>t_E</code>: The Einstein crossing time.</li> <li><code>rho</code>: The source radius normalized to the Einstein radius.</li> <li><code>q</code>: The planet to host mass ratio of the binary lens system.</li> <li><code>s</code>: The projected separation of the binary lens system normalized to the Einstein radius.</li> <li><code>alpha_deg</code>: The angle between the source trajectory and the binary axis in degrees.</li> <li><code>times</code>: The times at which to compute the model.</li> <li><code>tol</code>: The tolerance for the adaptive contour integration. Defaults to 1e-2.</li> <li><code>retol</code>: The relative tolerance for the adaptive contour integration. Defaults to 0.001.</li> <li><code>default_strategy</code>: The default strategy for the contour integration. Defaults to (30, 30, 60, 120, 240). more details can be found in the <code>microlux.contour_integral</code>.</li> <li><code>analytic</code>: Whether to use the analytic chain rule to simplify the computation graph. Set this to True will accelerate the computation of the gradient and will support the reverse mode differentiation containing the while loop. But set this to True will slow down if only calculate the model without differentiation. Defaults to True.</li> <li><code>return_info</code>: Whether to return additional information about the computation. Defaults to False.</li> <li><code>limb_darkening_coeff</code>: The limb darkening coefficient for the source star. Defaults to None. currently only support linear limb darkening.</li> </ul> <p>Returns</p> <ul> <li><code>magnification</code>: The magnification of the source at the given times.</li> <li><code>info</code>: Additional information about the computation used for debugging if return_info is True.</li> </ul>"},{"location":"api/model/#microlux.point_light_curve","title":"<code>microlux.point_light_curve(trajectory_l, s, q, rho, tol, return_num: bool = False)</code>","text":"<p>Calculate the point source light curve.</p> <p>Parameters</p> <ul> <li><code>trajectory_l</code>: The trajectory of the lensing event.</li> <li><code>s</code>: The projected separation between the lens and the source.</li> <li><code>q</code> : The mass ratio between the lens and the source.</li> <li><code>rho</code>: The source radius in units of the Einstein radius.</li> <li><code>tol</code>: The absolute tolerance for the quadrupole test.</li> <li><code>return_num</code>: Whether to return the number of real roots. Defaults to False.</li> </ul> <p>Returns</p> <ul> <li><code>result</code>: A tuple containing:<ul> <li>The magnification array.</li> <li>A boolean array indicating the validity of the calculation. If the quadrupole test is passed, the corresponding element in the boolean array is <code>True</code>.</li> <li>If <code>return_num</code> is <code>True</code>, the tuple will also contain the number of real roots.</li> </ul> </li> <li><code>cond</code>: A boolean array indicating whether the quadrupole test is passed. <code>True</code> means the quadrupole test is passed.</li> <li><code>mask</code>: An integer array indicating the number of real roots.</li> </ul>"},{"location":"api/model/#microlux.extended_light_curve","title":"<code>microlux.extended_light_curve(trajectory_l, s, q, rho, tol=0.01, retol=0.001, default_strategy: Tuple[int] = (30, 30, 60, 120, 240), analytic: bool = True, return_info: bool = False, limb_darkening: AbstractLimbDarkening | None = None, n_annuli: int = 10)</code>","text":"<p>compute the light curve of a binary lens system with finite source effects.</p> <p>Parameters</p> <ul> <li><code>trajectory_l</code>: The trajectory in the low mass coordinate system.</li> <li><code>n_annuli</code>: The number of annuli for the limb darkening calculation.</li> <li>for the definition of the other parameters, please see <code>microlux.binary_mag</code>.</li> </ul>"},{"location":"api/model/#microlux.contour_integral","title":"<code>microlux.contour_integral(trajectory_l, tol, retol, rho, s, q, default_strategy=(60, 80, 150), analytic=True) -&gt; Tuple[jnp.ndarray, Tuple]</code>","text":"<p>Perform adaptive contour integration with pre-define shaped array. This function is used to reduce the memory usage and improve the performance of the contour integration. The reason is that the optimal fixed array length is hard to determine before the code runs which the basic requirement for JIT compilation. If the array length is too small, the adaptive contour integration will stop early and the error will be larger than the tolerance. If the array length is too large, it will cause the waste of memory and time. This waste is linear with the array length. So we use this pre-define shaped array to solve this problem.</p> <p>Parameters - For other parameters: please see at <code>microlux.binary_mag</code> - <code>default_strategy</code>: The default strategy for the contour integration. The array length will be added gradually according to this strategy. For example, if the default_strategy is (60, 80, 150), the array length in each layer will be 60, 140, 290, respectively. - <code>analytic</code>: Whether to use the analytic chain rule to simplify the computation graph. Set this to True will accelerate the computation of the gradient and will support the reverse mode differentiation containing the while loop. But set this to True will slow down if only calculate the model without differentiation. Defaults to True.</p> <p>Returns - result: A tuple containing the magnitude and the result of the contour integration.</p>"},{"location":"api/solver/","title":"Solver","text":""},{"location":"api/solver/#root-solver","title":"Root Solver","text":""},{"location":"api/solver/#microlux.polynomial_solver.Aberth_Ehrlich","title":"<code>microlux.polynomial_solver.Aberth_Ehrlich(coff: jnp.ndarray, roots: jnp.ndarray, MAX_ITER: int = 50) -&gt; jnp.ndarray</code>","text":"<p>Solves a polynomial equation using the Aberth-Ehrlich method. Adapted from https://github.com/afoures/aberth-method. Use <code>jax.lax.custom_root</code> to get precise derivative in automatic differentiation.</p> <p>Parameters:</p> <ul> <li><code>coff</code>: Coefficients of the polynomial equation.</li> <li><code>roots</code>: Initial guesses for the roots of the polynomial equation.</li> <li><code>MAX_ITER</code>: Maximum number of iterations. Defaults to 100.</li> </ul> <p>Returns:</p> <ul> <li><code>roots</code>: The roots of the polynomial equation.</li> </ul>"},{"location":"api/solver/#linear_sum_assignment-solver","title":"linear_sum_assignment solver","text":""},{"location":"api/solver/#microlux.linear_sum_assignment.solve","title":"<code>microlux.linear_sum_assignment.solve(cost)</code>","text":"<p>Solves the linear sum assignment problem using the Hungarian algorithm. Adapted from https://github.com/google/jax/issues/10403</p> <p>Parameters: - <code>cost</code>: The cost matrix representing the assignment problem.</p> <p>Returns:</p> <ul> <li><code>row_ind</code>: The row indices of the assigned elements.</li> <li><code>col_ind</code>: The column indices of the assigned elements.</li> </ul>"},{"location":"api/utils/","title":"Utils","text":"<p>Here are some utility classes that are used in the microlux library.</p>"},{"location":"api/utils/#classes","title":"Classes","text":""},{"location":"api/utils/#microlux.Iterative_State","title":"<code>microlux.Iterative_State</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A NamedTuple representing the state of an iterative process in the microlux module.</p> <p>Attributes:</p> Name Type Description <code>sample_num</code> <code>int</code> <p>The number of samples.</p> <code>theta</code> <code>Array</code> <p>The sampling angles.</p> <code>roots</code> <code>Array</code> <p>The roots array.</p> <code>parity</code> <code>Array</code> <p>The parity array.</p> <code>ghost_roots_distant</code> <code>Array</code> <p>The ghost roots distant array, used to detect the buried images (hidden cusps)</p> <code>sort_flag</code> <code>Union[bool, Array]</code> <p>A boolean flag indicating whether the roots are sorted and matched.</p> <code>Is_create</code> <code>Array</code> <p>The Is_create array. A boolean flag indicating whether the image is created or destroyed.</p>"},{"location":"api/utils/#microlux.Error_State","title":"<code>microlux.Error_State</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Error_State is a NamedTuple that holds various attributes related to the error state in the adaptive sampling.</p> <p>Attributes:</p> Name Type Description <code>mag</code> <code>Array</code> <p>Current magnification values.</p> <code>mag_no_diff</code> <code>int</code> <p>The number of magnification values without sufficient difference.</p> <code>outloop</code> <code>int</code> <p>A integer flag indicating whether the iteration should be terminated.</p> <code>error_hist</code> <code>Array</code> <p>The current error estimated in each sampling interval.</p> <code>epsilon</code> <code>float</code> <p>The absolute tolerance value.</p> <code>epsilon_rel</code> <code>float</code> <p>The relative tolerance value.</p> <code>exceed_flag</code> <code>bool</code> <p>The flag indicating whether the current sampling number exceeds the length of the array.</p>"},{"location":"example/KB0371/","title":"KB-19-0371","text":"<p>This notebook shows the application of our code to the real event analysis including NUTS, Fisher matrix and Basin-hopping optimization.</p> <pre><code>import numpy as np\n\n# %matplotlib ipympl\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport os\nglobal numofchains\nglobal N_pmap\nN_pmap = 10\nnumofchains = 1\nos.environ[\"XLA_FLAGS\"] = f'--xla_force_host_platform_device_count={N_pmap*numofchains}'\nimport jax\nimport jax.numpy as jnp\n\nimport jax\nimport jax.numpy as jnp\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\nfrom numpyro.diagnostics import print_summary\nfrom microlux import binary_mag\n\nimport VBBinaryLensing\nfrom MulensModel import caustics\nfrom IPython.display import display, clear_output\n\nimport emcee\nimport corner\nfrom multiprocessing import Pool\n\nprint(os.getcwd())\ndata = pd.read_csv('microlensing/example/data/KB_19_0371.csv') ## remind to change the path to the data file\n\ncond = (data['e_mag'] &amp;lt; 0.4) &amp;amp; (data['HJD'] &amp;gt; 8500)\ndata = data[cond]\n\nerror_frac = {'OGLE':1.59, 'KMTC01':1.41, 'KMTC41':1.38, 'KMTA01':1.35, 'KMTA41':1.57, 'KMTS01':1.19, 'KMTS41':1.41}\ndata['e_mag'] = data.apply(lambda x: np.sqrt(0.003**2+x['e_mag']**2*error_frac[x['Tel']]**2), axis=1)\n\nfs_dict = {'OGLE':0.1865329, 'KMTC01':0.15551681, 'KMTC41':0.16063666, 'KMTA01':0.1964294, 'KMTA41':0.12068191, 'KMTS01':0.22724801, 'KMTS41':0.16661919}\nfb_dict = {'OGLE':0.07354933, 'KMTC01':0.10144077, 'KMTC41':0.10602545, 'KMTA01':0.04612094, 'KMTA41':0.144712, 'KMTS01':0.00623068, 'KMTS41':0.09311172}\ndef align_function(mag, mag_err, fs, fb, fs_ogle, fb_ogle):\n    flux = 10.**(0.4*(18.-mag))\n    ferr = mag_err*flux*np.log(10.)/2.5\n\n    flux_ogle = (flux-fb)/fs*fs_ogle+fb_ogle\n    ferr_ogle = ferr/fs*fs_ogle\n\n    mag_ogle = 18.-2.5*np.log10(flux_ogle)\n    mag_err_ogle = ferr_ogle/flux_ogle*2.5/np.log(10.)\n    return mag_ogle, mag_err_ogle\ndata['mag_aligned'], data['e_mag_aligned'] = zip(*data.apply(lambda x: align_function(x['mag'], x['e_mag'], fs_dict[x['Tel']], fb_dict[x['Tel']], fs_dict['OGLE'], fb_dict['OGLE']), axis=1))\n\ndata\n</code></pre> <pre>\n<code>/home/coast/Documents/astronomy/microlensing\n</code>\n</pre> Tel Filter HJD mag e_mag mag_aligned e_mag_aligned 1894 OGLE I 8521.87056 19.553 0.114519 19.553000 0.114519 1895 OGLE I 8522.86293 19.610 0.120877 19.610000 0.120877 1896 OGLE I 8523.86950 19.536 0.120877 19.536000 0.120877 1897 OGLE I 8526.85876 19.367 0.119288 19.367000 0.119288 1898 OGLE I 8529.86680 19.515 0.143131 19.515000 0.143131 ... ... ... ... ... ... ... ... 12227 KMTS41 I 8776.25067 19.543 0.080426 19.551285 0.090728 12228 KMTS41 I 8776.27388 19.594 0.094518 19.608994 0.107285 12229 KMTS41 I 8776.28745 19.412 0.100155 19.404592 0.111363 12230 KMTS41 I 8777.24100 19.487 0.114249 19.488316 0.128059 12231 KMTS41 I 8777.28137 19.415 0.102974 19.407928 0.114532 <p>10287 rows \u00d7 7 columns</p> <pre><code>cond = (data['e_mag_aligned'] &amp;lt; 0.4) &amp;amp; (data['HJD'] &amp;gt; 8500)\ndata = data[cond]\n# error_frac = {'OGLE':1.59, 'KMTC01':1.41, 'KMTC41':1.38, 'KMTA01':1.35, 'KMTA41':1.57, 'KMTS01':1.19, 'KMTS41':1.41}\n# data['e_mag_aligned'] = data.apply(lambda x: np.sqrt(0.003**2+x['e_mag_aligned']**2*error_frac[x['Tel']]**2), axis=1)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nall_tel = data['Tel'].unique()\nfor i in all_tel:\n    tel_data = data[data['Tel'] == i]\n    ax.errorbar(tel_data['HJD'], tel_data['mag_aligned'], yerr=tel_data['e_mag_aligned'], fmt='o', label=i)\nax.set_xlabel('HJD')\nax.set_ylabel('Magnitude')\nax.legend()\n\nfig.gca().invert_yaxis()\nplt.show()\n</code></pre> <p>There are two degenrate solutions for this event. The wide/close degeneracy light curve is plotted in below. We use the close solution as the example.</p> <pre><code>def mag_to_flux(mag, e_mag):\n    flux = 10.**(0.4*(18.-mag))\n    ferr = e_mag*flux*np.log(10.)/2.5\n    return flux, ferr\ndef flux_to_mag(flux):\n    mag = 18.-2.5*np.log10(flux)\n    return mag\ndef light_curve_VBBL(times,parms):\n    t0 = parms['t0']\n    u0 = parms['u0']\n    tE = parms['tE']\n    rho = 10.**parms['logrho']\n    alpha_deg = parms['alpha']\n    s = 10.**parms['logs']\n    q = 10.**parms['logq']\n    tau = (times-t0)/tE\n    VBBL = VBBinaryLensing.VBBinaryLensing()\n    alpha_VBBL=alpha_deg/180*np.pi+np.pi\n    VBBL.Tol=1e-2\n    VBBL.RelTol=1e-3\n    VBBL.BinaryLightCurve\n    y1 = -u0*np.sin(alpha_VBBL) + tau*np.cos(alpha_VBBL)\n    y2 = u0*np.cos(alpha_VBBL) + tau*np.sin(alpha_VBBL)\n    params = [np.log(s), np.log(q), u0, alpha_VBBL, np.log(rho), np.log(tE), t0]\n    VBBL_mag = VBBL.BinaryLightCurve(params, times, y1, y2)\n    return np.array(VBBL_mag)\n\nparms_close = {'t0': 8592.388619, 'u0': 0.140631, 'tE': 6.655161, 'logrho': -2.231148, 'alpha': 271.695690, 'logs': -0.079158, 'logq': -1.141006}\n# parms_wide = {'t0': 8592.391925, 'u0': 0.144696, 'tE': 6.640740, 'logrho': -2.187052, 'alpha': 271.325666, 'logs': 0.188680, 'logq': -0.957499}\n\nflux,ferr = mag_to_flux(data['mag_aligned'].values, data['e_mag_aligned'].values)\nHJD = data['HJD'].values\nfs,fb = 0.18893952,0.07114746\n\ntimes = np.linspace(8500, 8800, 2000)\nmag_close = light_curve_VBBL(times, parms_close)\nflux_close = mag_close*fs + fb\nmag_close = flux_to_mag(flux_close)\n\nax.plot(times, mag_close,label='Close solution')\nax.legend()\nax.set_xlim(8580, 8600)\n\nax_traj = fig.add_axes([0.2, 0.6, 0.25, 0.25])\ntau = (times-parms_close['t0'])/parms_close['tE']\nalpha = parms_close['alpha']/180*np.pi\ny1 = -parms_close['u0']*np.sin(alpha) + tau*np.cos(alpha)\ny2 = parms_close['u0']*np.cos(alpha) + tau*np.sin(alpha)\nax_traj.plot(y1, y2,c='black')\nax_traj.set_aspect('equal')\nax_traj.set_xlim(-1., 1.)\nax_traj.set_ylim(-1., 1.)\ncaustics_instance = caustics.Caustics(s=10**parms_close['logs'], q=10**parms_close['logq'])\ncaustics_x, caustics_y = caustics_instance.get_caustics()\nax_traj.scatter(caustics_x, caustics_y, c='r', s=1)\n\ndisplay(fig)\n</code></pre> <pre><code>def objective_func(parms, data, fs, fb, return_chi2=True):\n    parm_name = ['t0', 'u0', 'tE', 'logrho', 'alpha', 'logs', 'logq']\n    parm_dict = dict(zip(parm_name, parms))\n    times,flux,ferr = data\n    model_flux = light_curve_VBBL(times, parm_dict)*fs + fb\n    chi2 = np.sum(((model_flux-flux)/ferr)**2)\n    if return_chi2:\n        return chi2\n    else:\n        return -0.5*chi2\ninitial_guess = [8.59238794e+03, 1.42915228e-01, 6.61567944e+00, -2.23131913e+00, 2.71714918e+02, -7.73128397e-02, -1.14229367e+00]\nprint(objective_func(initial_guess, [HJD,flux,ferr], fs, fb))\nprint('tot dof = ', len(HJD)-len(parms_close))\n</code></pre> <pre>\n<code>10225.764464996253\ntot dof =  10161\n</code>\n</pre> <pre><code># import scipy.optimize as op\n# res = op.minimize(objective_func, x0=initial_guess, args=([HJD,flux,ferr], fs, fb), method='Nelder-Mead')\n# print(res.x)\n# print(res.fun)\n</code></pre> <pre><code>if __name__ == '__main__':\n    n_dim = len(initial_guess)\n    nwalkers = 20\n    step_size = 0.001*np.ones_like(initial_guess)\n    pos = [initial_guess+step_size*np.random.randn(n_dim) for i in range(nwalkers)] \n    with Pool(nwalkers) as pool:\n        sampler = emcee.EnsembleSampler(nwalkers, n_dim, objective_func, args=([HJD,flux,ferr], fs, fb, False), pool=pool)\n        pos, prob, state = sampler.run_mcmc(pos, 500, progress=True)\n        sampler.reset()\n        sampler.run_mcmc(pos, 1000, progress=True)\n</code></pre> <pre>\n<code>/home/coast/miniconda3/envs/autodiff/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [00:43&lt;00:00, 11.53it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:30&lt;00:00, 11.02it/s]\n</code>\n</pre> <pre><code>sample_chain = sampler.get_chain()\nprint(sample_chain.shape)\n\nsample_chain_reshape = jnp.transpose(sample_chain, (1, 0, 2))\n\nprint(sample_chain_reshape.shape)\nprint_summary(sample_chain_reshape)\n</code></pre> <pre>\n<code>(1000, 20, 7)\n(20, 1000, 7)\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\nParam:0[0]   8592.39      0.01   8592.39   8592.38   8592.40    192.50      1.10\nParam:0[1]      0.14      0.00      0.14      0.14      0.14    208.35      1.11\nParam:0[2]      6.62      0.03      6.62      6.56      6.67    239.32      1.09\nParam:0[3]     -2.23      0.01     -2.23     -2.24     -2.22    251.92      1.08\nParam:0[4]    271.67      0.41    271.67    271.01    272.33    181.92      1.11\nParam:0[5]     -0.08      0.00     -0.08     -0.08     -0.08    215.96      1.09\nParam:0[6]     -1.14      0.01     -1.14     -1.16     -1.13    210.24      1.09\n\n</code>\n</pre> <pre><code>parm_name = ['t0', 'u0', 'tE', 'logrho', 'alpha', 'logs', 'logq']\nchain = sampler.get_chain(flat=True)\nfig = corner.corner(chain,labels=parm_name,quantiles=[0.16, 0.5, 0.84],show_titles=True,truths=np.median(chain,axis=0))\nplt.show()\nfor i in range(len(parm_name)):\n    print(parm_name[i], np.median(chain[:,i]), np.std(chain[:,i]))\n</code></pre> <pre>\n<code>t0 8592.388418845861 0.0064818587784778794\nu0 0.14286323570770812 0.0009629699096971133\ntE 6.616800447339974 0.033837046463837665\nlogrho -2.2296836689929522 0.008402443064801798\nalpha 271.6881009657352 0.4458504726993328\nlogs -0.07733778479058154 0.001033052615132573\nlogq -1.1422378219044174 0.007720610229842566\n</code>\n</pre> <pre><code>from matplotlib.ticker import MaxNLocator\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib.patches import Ellipse\nimport matplotlib.cm as cm\ndef hist2d(x, y, *args, **kwargs):\n    \"\"\"\n    Plot a 2-D histogram of samples.\n\n    \"\"\"\n    ax = kwargs.pop(\"ax\", plt.gca())\n\n    extent = kwargs.pop(\"extent\", [[x.min(), x.max()], [y.min(), y.max()]])\n    bins = kwargs.pop(\"bins\", 30)\n    color = kwargs.pop(\"color\", \"b\")\n    linewidths = kwargs.pop(\"linewidths\", None)\n    plot_datapoints = kwargs.get(\"plot_datapoints\", True)\n    plot_contours = kwargs.get(\"plot_contours\", True)\n\n    cmap=plt.get_cmap(\"gray\")\n    cmap._init()\n    cmap._lut[:-3, :-1] = 0.\n    cmap._lut[:-3, -1] = np.linspace(1, 0, cmap.N)\n\n    X = np.linspace(extent[0][0], extent[0][1], bins + 1)\n    Y = np.linspace(extent[1][0], extent[1][1], bins + 1)\n    try:\n        H, X, Y = np.histogram2d(x.flatten(), y.flatten(), bins=(X, Y),\n                                 weights=kwargs.get('weights', None))\n    except ValueError:\n        raise ValueError(\"It looks like at least one of your sample columns \"\n                         \"have no dynamic range. You could try using the \"\n                         \"`extent` argument.\")\n\n    V = 1.0 - np.exp(-0.5 * np.arange(1, 3.1, 1) ** 2)\n    Hflat = H.flatten()\n    inds = np.argsort(Hflat)[::-1]\n    Hflat = Hflat[inds]\n    sm = np.cumsum(Hflat)\n    sm /= sm[-1]\n\n    for i, v0 in enumerate(V):\n        try:\n            V[i] = Hflat[sm &amp;lt;= v0][-1]\n        except:\n            V[i] = Hflat[0]\n\n    X1, Y1 = 0.5 * (X[1:] + X[:-1]), 0.5 * (Y[1:] + Y[:-1])\n    X, Y = X[:-1], Y[:-1]\n\n    if plot_datapoints:\n        ax.plot(x, y, \"o\", color=color, ms=1.5, zorder=-1, alpha=0.1,\n                rasterized=True)\n        if plot_contours:\n            ax.contourf(X1, Y1, H.T, [V[-1], H.max()],\n                        cmap=LinearSegmentedColormap.from_list(\"cmap\",\n                                                               ([1] * 3,\n                                                                [1] * 3),\n                        N=2),antialiased=False)\n\n    if plot_contours:\n#        ax.pcolor(X, Y, H.max() - H.T, cmap=cmap)\n        V = [V[-1],V[-2],V[-3]]\n        ax.contour(X1, Y1, H.T, V, colors=color, alpha=0.5,linewidths=linewidths)\n#        ax.contourf(X1, Y1, H.T, [V[-1], H.max()], cmap=LinearSegmentedColormap.from_list(\"cmap\",([1] * 3,[1] * 3),N=2), antialiased=False)\n\n    data = np.vstack([x, y])\n    mu = np.mean(data, axis=1)\n    cov = np.cov(data)\n    if kwargs.pop(\"plot_ellipse\", False):\n        error_ellipse(mu, cov, ax=ax, edgecolor=\"r\", ls=\"dashed\")\n\n    ax.set_xlim(extent[0])\n    ax.set_ylim(extent[1])\n    #ax.set_xticklabels([])\n    #ax.set_yticklabels([])\n    return\n\n\ndef plot_covariance(params,labels,cov_mat,chain):\n    ''' plot covariance matrix: both theoretical &amp;amp; mcmc chain. '''\n    ## set up axes ##\n    K = len(params)\n    factor = 2.0           # size of one side of one panel\n    lbdim = 1.2 * factor   # size of left/bottom margin\n    trdim = 0.15 * factor  # size of top/right margin\n    whspace = 0.1         # w/hspace size\n    plotdim = factor * K + factor * (K - 1.) * whspace\n    dim = lbdim + plotdim + trdim\n    fig,axes = plt.subplots(K,K,figsize=(10,10))\n    lb = lbdim / dim\n    tr = (lbdim + plotdim) / dim\n    fig.subplots_adjust(left=lb, bottom=lb, right=tr, top=tr, wspace=whspace, hspace=whspace)\n    ## set up axex extent ##\n    extents = [[x.min(), x.max()] for x in chain.T]\n    ##\n    for i in range(K):\n        ax = axes[i,i]\n        mu_x,sigma_x = params[i],np.sqrt(cov_mat[i,i])\n        x = np.linspace(extents[i][0],extents[i][1],100)\n        p = 1/np.sqrt(2*np.pi)/sigma_x * np.exp(-(x-mu_x)**2/2./sigma_x**2)\n        ax.plot(x,p,'r',alpha=0.5)\n        ax.hist(chain[:,i],histtype='step',density=1)\n        ax.set_xlim(extents[i])\n        ax.set_yticklabels([])\n        ax.xaxis.set_major_locator(MaxNLocator(4))\n        if i &amp;lt; K-1:\n            ax.set_xticklabels([])\n        else:\n            [l.set_rotation(45) for l in ax.get_xticklabels()]\n            if labels is not None:\n                ax.set_xlabel(labels[i])\n                ax.xaxis.set_label_coords(0.5, -0.7)\n        for j in range(K):\n            ax = axes[i,j]\n            if j &amp;gt; i:\n                ax.set_visible(False)\n                ax.set_frame_on(False)\n                continue\n            elif j == i:\n                continue\n            ## plot error ellipse from given covariance matrix ##\n            mu_y,sigma_y = params[j],np.sqrt(cov_mat[j,j])\n            sigx2,sigy2,sigxy = cov_mat[i,i],cov_mat[j,j],cov_mat[i,j]\n            ## find principle axes ##\n            sig12 = 0.5*(sigx2+sigy2) + np.sqrt((sigx2-sigy2)**2*0.25+sigxy**2)\n            sig22 = 0.5*(sigx2+sigy2) - np.sqrt((sigx2-sigy2)**2*0.25+sigxy**2)\n            sig1 = np.sqrt(sig12)\n            sig2 = np.sqrt(sig22)\n            alpha = 0.5*np.arctan(2*sigxy/(sigx2-sigy2))\n            if sigy2 &amp;gt; sigx2:\n                alpha += np.pi/2.\n            ## plot ellipse ##\n            t = np.linspace(0,2*np.pi,300)\n            x = mu_x + sig1*np.cos(t)*np.cos(alpha) - sig2*np.sin(t)*np.sin(alpha)\n            y = mu_y + sig1*np.cos(t)*np.sin(alpha) + sig2*np.sin(t)*np.cos(alpha)\n            ax.plot(y,x,'r',alpha=0.5)\n            ## plot error ellipse from mcmc chain ##\n            hist2d(chain[:,j],chain[:,i],ax=ax,extent=[extents[j],extents[i]],plot_contours=True,plot_datapoints=False)\n            ax.xaxis.set_major_locator(MaxNLocator(4))\n            ax.yaxis.set_major_locator(MaxNLocator(4))\n            if i &amp;lt; K-1:\n                ax.set_xticklabels([])\n            else:\n                [l.set_rotation(45) for l in ax.get_xticklabels()]\n                if labels is not None:\n                    ax.set_xlabel(labels[j])\n                    ax.xaxis.set_label_coords(0.5,-0.7)\n            if j &amp;gt; 0:\n                ax.set_yticklabels([])\n            else:\n                [l.set_rotation(45) for l in ax.get_yticklabels()]\n                if labels is not None:\n                    ax.set_ylabel(labels[i])\n                    ax.yaxis.set_label_coords(-0.6,0.5)\n    return fig,axes\n</code></pre> <pre><code>## fisher information matrix\n\ndef light_curve_Jax(parms,times):\n    t0 = parms[0]\n    u0 = parms[1]\n    tE = parms[2]\n    rho = 10.**parms[3]\n    alpha_deg = parms[4]\n    s = 10.**parms[5]\n    q = 10.**parms[6]\n    mag_Jax = binary_mag(t0, u0, tE, rho, q, s, alpha_deg, times)\n    return mag_Jax\ninitial_guess = [8.59238794e+03, 1.42915228e-01, 6.61567944e+00, -2.23131913e+00, 2.71714918e+02, -7.73128397e-02, -1.14229367e+00]\ntimes,flux,ferr = HJD,flux,ferr\nweight_light_curve = lambda x: (light_curve_Jax(x, times)*fs+fb)/ferr\njacobian_fun = jax.jacfwd(weight_light_curve)\njacobian = jacobian_fun(jnp.array(initial_guess))\nfisher_matrix = jnp.dot(jacobian.T, jacobian)\nfisher_cov = jnp.linalg.inv(fisher_matrix)\nfig,axes= plot_covariance(initial_guess,parm_name,fisher_cov,chain)\nplt.show()\n</code></pre> <pre><code>print(HJD.shape)\nHJD_pad = jnp.pad(HJD, (0, 10170-HJD.shape[0]), 'constant', constant_values=HJD[-1])\nprint(HJD_pad.shape)\nflux_pad = jnp.pad(flux, (0, 10170-flux.shape[0]), 'constant', constant_values=flux[-1])\nferr_pad = jnp.pad(ferr, (0, 10170-ferr.shape[0]), 'constant', constant_values=ferr[-1])\n</code></pre> <pre>\n<code>(10168,)\n(10170,)\n</code>\n</pre> <pre><code>def light_curve_Jax(times,parms):\n    t0 = parms[0]\n    u0 = parms[1]\n    tE = parms[2]\n    rho = 10.**parms[3]\n    alpha_deg = parms[4]\n    s = 10.**parms[5]\n    q = 10.**parms[6]\n    mag_Jax = binary_mag(t0, u0, tE, rho, q, s, alpha_deg, times)\n    return mag_Jax\ndef light_curve_Jax_pmap(times,parms,i):\n    times = jnp.reshape(times,(-1,N_pmap),order='C')\n    times_i = times[:,i]\n    return light_curve_Jax(times_i,parms)\n# def objective_func(parms, data, fs, fb, return_chi2=True):\n#     times,flux,ferr = data\n#     model_flux = light_curve_Jax(times, parms)*fs + fb\n#     chi2 = np.sum(((model_flux-flux)/ferr)**2)\n#     if return_chi2:\n#         return chi2\n#     else:\n#         return -0.5*chi2\n# print(objective_func(initial_guess, [HJD,flux,ferr], fs, fb))\n# print('tot dof = ', len(HJD)-len(parms_close))\ndef model_HMC(data, fs, fb, init_val, L):\n    times,flux,ferr = data\n    parmsample=numpyro.sample('param_base',dist.Uniform(-1*jnp.ones(len(init_val)),1*jnp.ones(len(init_val))))\n    parmsample=jnp.dot(L*10,parmsample)+jnp.array(init_val)\n    numpyro.deterministic('param',parmsample)\n    mag_mod = jax.pmap(light_curve_Jax_pmap,in_axes=(None,None,0))(times,parmsample,jnp.arange(10))\n    mag_mod = jnp.reshape(mag_mod,(flux.shape[0],),order='F')\n    flux_mod = mag_mod*fs + fb\n    # flux_mod = light_curve_Jax(times, parmsample)*fs + fb\n    numpyro.sample('obs', dist.Normal(flux_mod, ferr), obs=flux)\n    chi2 = jnp.sum(((flux_mod-flux)/ferr)**2)\n    numpyro.deterministic('chi2',chi2)\n\nL = jnp.linalg.cholesky(fisher_cov)\n\ninit_strategy=numpyro.infer.init_to_value(values={'param_base':jnp.zeros(len(initial_guess))})\nnuts_kernel = NUTS(model_HMC,step_size=1e-2,target_accept_prob=0.8,init_strategy=init_strategy,forward_mode_differentiation=True)\nmcmc = MCMC(nuts_kernel,num_warmup=500,num_samples=1000,num_chains=1,progress_bar=True)\n\nmcmc.run(jax.random.PRNGKey(0),data=[HJD_pad,flux_pad,ferr_pad],fs=fs,fb=fb,init_val=initial_guess,L=L)\nmcmc.print_summary(exclude_deterministic=False)\n</code></pre> <pre>\n<code>  0%|          | 0/1500 [00:00&lt;?, ?it/s]/home/coast/miniconda3/envs/autodiff/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:1829: UserWarning: The jitted function _body_fn includes a pmap. Using jit-of-pmap can lead to inefficient data movement, as the outer jit does not preserve sharded data representations and instead collects input and output arrays onto a single device. Consider removing the outer jit unless you know what you're doing. See https://github.com/google/jax/issues/2926.\n  warnings.warn(\nwarmup:   0%|          | 1/1500 [00:43&lt;18:02:38, 43.33s/it, 63 steps of size 1.43e-01. acc. prob=1.00]/home/coast/miniconda3/envs/autodiff/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:1829: UserWarning: The jitted function _body_fn includes a pmap. Using jit-of-pmap can lead to inefficient data movement, as the outer jit does not preserve sharded data representations and instead collects input and output arrays onto a single device. Consider removing the outer jit unless you know what you're doing. See https://github.com/google/jax/issues/2926.\n  warnings.warn(\nsample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1500/1500 [10:48&lt;00:00,  2.31it/s, 3 steps of size 6.35e-01. acc. prob=0.92] \n/home/coast/miniconda3/envs/autodiff/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:1829: UserWarning: The jitted function &lt;unnamed wrapped function&gt; includes a pmap. Using jit-of-pmap can lead to inefficient data movement, as the outer jit does not preserve sharded data representations and instead collects input and output arrays onto a single device. Consider removing the outer jit unless you know what you're doing. See https://github.com/google/jax/issues/2926.\n  warnings.warn(\n</code>\n</pre> <pre>\n<code>\n                   mean       std    median      5.0%     95.0%     n_eff     r_hat\n         chi2  10233.04      3.65  10232.52  10227.54  10238.19    537.69      1.00\n     param[0]   8592.39      0.01   8592.39   8592.38   8592.40   1589.45      1.00\n     param[1]      0.14      0.00      0.14      0.14      0.14   1618.79      1.00\n     param[2]      6.62      0.03      6.62      6.56      6.67   1562.57      1.00\n     param[3]     -2.23      0.01     -2.23     -2.25     -2.22   1450.13      1.00\n     param[4]    271.71      0.42    271.72    271.05    272.40   1533.68      1.00\n     param[5]     -0.08      0.00     -0.08     -0.08     -0.08   1858.51      1.00\n     param[6]     -1.14      0.01     -1.14     -1.16     -1.13   1910.64      1.00\nparam_base[0]      0.00      0.10      0.00     -0.16      0.16   1589.45      1.00\nparam_base[1]     -0.01      0.10     -0.01     -0.19      0.14   1547.47      1.00\nparam_base[2]      0.00      0.10      0.01     -0.15      0.17   1867.11      1.00\nparam_base[3]      0.00      0.10      0.01     -0.17      0.16   1428.16      1.00\nparam_base[4]      0.02      0.10      0.01     -0.16      0.16   1348.62      1.00\nparam_base[5]      0.00      0.10      0.00     -0.15      0.16   2279.91      1.00\nparam_base[6]      0.00      0.09     -0.00     -0.15      0.15   1522.74      1.00\n\nNumber of divergences: 0\n</code>\n</pre> <pre><code>import corner\nhmc_sample = mcmc.get_samples()['param']\nprint(hmc_sample.shape)\nfig = corner.corner(np.array(hmc_sample),quantiles=[0.16, 0.5, 0.84],show_titles=True)\n</code></pre> <pre>\n<code>(1000, 7)\n</code>\n</pre>"}]}